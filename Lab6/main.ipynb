{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Fundamentals of Artificial Intelligence***\n",
    "\n",
    "> **Lab 6:** *Natural Language Processing and Chat Bots* <br>\n",
    "\n",
    "> **Performed by:** *Dan Hariton*, group *FAF-211* <br>\n",
    "\n",
    "> **Verified by:** Elena Graur, asist. univ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, T5ForConditionalGeneration\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from warnings import filterwarnings\n",
    "\n",
    "MODEL = 't5-base'\n",
    "SOURCE_LEN = 512\n",
    "TARGET_LEN = 512\n",
    "\n",
    "filterwarnings(\"ignore\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1\n",
    "\n",
    "Set up the Telegram Bot. Interact with BotFather on Telegram to obtain an API token. Create your Telegram Bot (its name should follow the pattern FIA_Surname_Name_FAF_21x). Make sure you are able to receive and send requests to it.\n",
    "\n",
    "1. Bot link: [FIA-Hariton-Dan-FAF-211](https://t.me/CityGuideHD_bot)\n",
    "\n",
    "2. Run `app.py` to start the bot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2\n",
    "\n",
    "Create a dataset that will serve as a training set for your model. It should follow the rules:\n",
    "- an entry consists of two parts: the question and the answer;\n",
    "- there are at least 75 entries written by you in your dataset;\n",
    "- questions should be something tourists or locals can ask about a new city.\n",
    "\n",
    "You can increase your dataset by adding open-source data. However, you MUST clearly show the questions written by you. Split your dataset into train and validation.\n",
    "\n",
    "*Hint: it is recommended to split it into 80% and 20%, but you can adjust it according to your needs.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"data.csv\")\n",
    "\n",
    "questions = dataset['question'].tolist()\n",
    "answers = dataset['answer'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3\n",
    "\n",
    "Use Tensorflow or Pytorch to implement the architecture of the Neural Network you are planning to use. It is highly recommended to use a Seq2Seq model (implement an LSTM or GRU architecture). You are NOT allowed to use pre-built or existing solutions (yep, connecting to GPT will not work)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "def tokenize(data, max_len):\n",
    "    return tokenizer(data, padding=True, truncation=True, return_tensors=\"pt\", max_length=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqDataset(Dataset):\n",
    "    def __init__(self, inputs, targets):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs['input_ids'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.inputs['input_ids'][idx],\n",
    "            'attention_mask': self.inputs['attention_mask'][idx],\n",
    "            'labels': self.targets['input_ids'][idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, questions_val, _, answers_val = train_test_split(questions, answers, test_size=0.2)\n",
    "\n",
    "tokenized_questions_train = tokenize(questions, SOURCE_LEN)\n",
    "tokenized_answers_train = tokenize(answers, TARGET_LEN)\n",
    "train_dataset = Seq2SeqDataset(tokenized_questions_train, tokenized_answers_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Save the used tokens to later check if the question is according to the tokens\n",
    "torch.save(tokenized_questions_train, 'models/used_tokens.pt')\n",
    "\n",
    "tokenized_questions_val = tokenize(questions_val, SOURCE_LEN)\n",
    "tokenized_answers_val = tokenize(answers_val, TARGET_LEN)\n",
    "val_dataset = Seq2SeqDataset(tokenized_questions_val, tokenized_answers_val)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4\n",
    "\n",
    "Train your model and fine-tune it based on the chosen performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqModel(nn.Module):\n",
    "    def __init__(self, model_name=MODEL):\n",
    "        super(Seq2SeqModel, self).__init__()\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels):\n",
    "        return self.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs: int = 10, file_name: str = 'model.pth'):\n",
    "    model = Seq2SeqModel().to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=5e-4)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        loop = tqdm(train_dataloader, leave=True)\n",
    "\n",
    "        for batch in loop:\n",
    "            loop.set_description(f\"Epoch {epoch}\")\n",
    "\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update progress bar\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    os.makedirs('models', exist_ok=True)\n",
    "    torch.save(model.state_dict(), f'models/{file_name}')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_loader):\n",
    "    model.eval()\n",
    "    total_bleu = 0\n",
    "    num_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        loop = tqdm(val_loader, leave=True)\n",
    "        for batch in loop:\n",
    "            loop.set_description(\"Evaluating: \")\n",
    "\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model.model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=TARGET_LEN)\n",
    "            predictions = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "            references = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "            for p, r in zip(predictions, references):\n",
    "                total_bleu += sentence_bleu([r.split()], p.split())\n",
    "                num_samples += 1\n",
    "\n",
    "    return total_bleu / num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/5 [00:00<?, ?it/s]Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "Epoch 0: 100%|██████████| 5/5 [00:16<00:00,  3.22s/it, loss=2.54]\n",
      "Epoch 1: 100%|██████████| 5/5 [00:12<00:00,  2.52s/it, loss=2.1] \n",
      "Epoch 2: 100%|██████████| 5/5 [00:12<00:00,  2.59s/it, loss=1.25]\n",
      "Epoch 3: 100%|██████████| 5/5 [00:12<00:00,  2.49s/it, loss=1.03] \n",
      "Epoch 4: 100%|██████████| 5/5 [00:12<00:00,  2.40s/it, loss=0.832]\n",
      "Epoch 5: 100%|██████████| 5/5 [00:11<00:00,  2.39s/it, loss=0.613]\n",
      "Epoch 6: 100%|██████████| 5/5 [00:11<00:00,  2.38s/it, loss=0.507]\n",
      "Epoch 7: 100%|██████████| 5/5 [00:12<00:00,  2.51s/it, loss=0.26] \n",
      "Epoch 8: 100%|██████████| 5/5 [00:12<00:00,  2.52s/it, loss=0.268]\n",
      "Epoch 9: 100%|██████████| 5/5 [00:11<00:00,  2.37s/it, loss=0.161]\n",
      "Evaluating: : 100%|██████████| 1/1 [00:01<00:00,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "TRAINED_MODEL = 'model10base.pth'\n",
    "\n",
    "model = train(10, TRAINED_MODEL)\n",
    "bleu_score = evaluate(model, val_dataloader)\n",
    "\n",
    "print(f\"BLEU Score: {bleu_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 5\n",
    "\n",
    "Integrate your model into your Telegram ChatBot, so that the sent messages are taken as input by the model and its output is sent back as a reply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_tokens(tokens):\n",
    "    saved_tokens = torch.load('models/used_tokens.pt')['input_ids']\n",
    "    return all([t in saved_tokens for t in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 363,   19,    8, 1784,   13, 3411,   58,    1])\n",
      "363: What: True\n",
      "19: is: True\n",
      "8: the: True\n",
      "1784: capital: False\n",
      "13: of: True\n",
      "3411: Japan: False\n",
      "58: ?: True\n",
      "1: </s>: True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Invalid tokens'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Seq2SeqModel().to(device)\n",
    "model.load_state_dict(torch.load(f'models/{TRAINED_MODEL}'))\n",
    "\n",
    "def generate_answer(question, model, tokenizer):\n",
    "    model.eval()\n",
    "    input_ids = tokenizer(question, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "    if not check_tokens(input_ids[0]):\n",
    "        print(input_ids[0])\n",
    "\n",
    "        for token in input_ids[0]:\n",
    "            print(f\"{token}: {tokenizer.decode(token)}: {check_tokens([token])}\")\n",
    "\n",
    "        return \"Invalid tokens\"\n",
    "\n",
    "    output = model.model.generate(input_ids, max_length=TARGET_LEN)\n",
    "\n",
    "    decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    return decoded\n",
    "\n",
    "# test_question = \"How do I get to the Hokage’s office?\"\n",
    "test_question = \"What is the capital of Japan?\"\n",
    "display(generate_answer(test_question, model, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was helped by Corneliu Catabluga from 213, he helped me with the T5 transformer, explaining to me how it works and how to use it along with Seq2Seq Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
